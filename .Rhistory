##train variables
train.input <- processed.data[train_index, ] %>% select('news')
train.label <- processed.data[train_index, ] %>% select('label')
train.data  <- processed.data[train_index, ]
##test variables
test.input <- processed.data[-train_index, ] %>% select('news')
test.label <- processed.data[-train_index, ] %>% select('label')
test.data  <- processed.data[-train_index, ]
# Mixed event models
dist <- fastNaiveBayes::fastNaiveBayes.detect_distribution(x, nrows = nrow(x))
print(dist)
mod <- fastNaiveBayes.mixed(x,y,laplace = 1)
pred <- predict(mod, newdata = x)
mean(pred!=y)
# Bernoulli only
vars <- c(dist$bernoulli, dist$multinomial)
newx <- x[,vars]
for(i in 1:ncol(newx)){
newx[[i]] <- as.factor(newx[[i]])
}
library(fastNaiveBayes)
###################################### DEFINING FUNCTIONS
#1 - PREPROCESSING FUNCTION - USED BY ALL MODELS
dataPreprocessing <- function(df) {
df_train <- data.frame('id' = df$tid2 ,
'news' = df$title2_en,
'label' = df$label);
df_train_unique <-  unique(df_train);
df_train_unique$label <- as.factor(df_train_unique$label);
train_label <- pivot_wider(df_train_unique,
id_cols = c('id','news'),
names_from = 'label',
values_from ='label');
train_label <- as.data.frame(train_label);
train_label$label <- NA;
#It provide the final label. If agreed->fake news, else not fake
for (i in 1:length(train_label$id)){
if(!is.na(train_label$agreed[i])) {
train_label$label[i] <-'fake';
}
else {
train_label$label[i] <-'not fake';
}
}
train_label$label[train_label$label=='not fake'] <- as.integer(0);
train_label$label[train_label$label=='fake'] <- as.integer(1);
train_label_final <- train_label[c('id', 'news','label')];
train_label_final$news <- as.character(train_label_final$news);
train_label_final$label  <- factor(train_label_final$label)
return(train_label_final);
}
#2-  Creating Corpus and Generating DTM - NAIVE BAYES AND LOGISTIC REGRESSION REQUIREMENT
createCorpusAndDTM <- function(dataset) {
nb_corpus <- VCorpus(VectorSource(dataset$news));
nb_corpus_clean <- tm_map(nb_corpus, content_transformer(tolower));
nb_corpus_clean <- tm_map(nb_corpus_clean, content_transformer(removeNumbers));
nb_corpus_clean <- tm_map(nb_corpus_clean, removePunctuation);
nb_corpus_clean <- tm_map(nb_corpus_clean, removeWords,stopwords());
nb_corpus_clean <- tm_map(nb_corpus_clean, stemDocument);
nb_corpus_clean <- tm_map(nb_corpus_clean, stripWhitespace);
doc_matrix = DocumentTermMatrix(nb_corpus_clean);
return(doc_matrix);
}
#3 -CONVERTING THE FACTOR FROM STRING TO INTEGER - NAIVE BAYES MODEL REQUIREMENT
convert_counts <- function(x){
x <- ifelse(x > 0, "Yes", "No")
}
###################################### PREPROCESSING - ALL MODELS
# fake_news_data = read.csv2('train-3.csv', sep = ',', stringsAsFactors = F)
fake_news_data = read_csv('Reduced Data/train-3.csv')
processed.data <- dataPreprocessing(fake_news_data)
processed.label <- processed.data %>% select('label')
#splitting the data and getting the variables ready for the models
smp_size <- floor(0.75 * nrow(processed.data))
train_index <- sample(seq_len(nrow(processed.data)), size = smp_size)
##train variables
train.input <- processed.data[train_index, ] %>% select('news')
train.label <- processed.data[train_index, ] %>% select('label')
train.data  <- processed.data[train_index, ]
##test variables
test.input <- processed.data[-train_index, ] %>% select('news')
test.label <- processed.data[-train_index, ] %>% select('label')
test.data  <- processed.data[-train_index, ]
setwd("~/MyProjects/News-Classifier-FAKE-or-NOT")
library(fastNaiveBayes)
###################################### DEFINING FUNCTIONS
#1 - PREPROCESSING FUNCTION - USED BY ALL MODELS
dataPreprocessing <- function(df) {
df_train <- data.frame('id' = df$tid2 ,
'news' = df$title2_en,
'label' = df$label);
df_train_unique <-  unique(df_train);
df_train_unique$label <- as.factor(df_train_unique$label);
train_label <- pivot_wider(df_train_unique,
id_cols = c('id','news'),
names_from = 'label',
values_from ='label');
train_label <- as.data.frame(train_label);
train_label$label <- NA;
#It provide the final label. If agreed->fake news, else not fake
for (i in 1:length(train_label$id)){
if(!is.na(train_label$agreed[i])) {
train_label$label[i] <-'fake';
}
else {
train_label$label[i] <-'not fake';
}
}
train_label$label[train_label$label=='not fake'] <- as.integer(0);
train_label$label[train_label$label=='fake'] <- as.integer(1);
train_label_final <- train_label[c('id', 'news','label')];
train_label_final$news <- as.character(train_label_final$news);
train_label_final$label  <- factor(train_label_final$label)
return(train_label_final);
}
#2-  Creating Corpus and Generating DTM - NAIVE BAYES AND LOGISTIC REGRESSION REQUIREMENT
createCorpusAndDTM <- function(dataset) {
nb_corpus <- VCorpus(VectorSource(dataset$news));
nb_corpus_clean <- tm_map(nb_corpus, content_transformer(tolower));
nb_corpus_clean <- tm_map(nb_corpus_clean, content_transformer(removeNumbers));
nb_corpus_clean <- tm_map(nb_corpus_clean, removePunctuation);
nb_corpus_clean <- tm_map(nb_corpus_clean, removeWords,stopwords());
nb_corpus_clean <- tm_map(nb_corpus_clean, stemDocument);
nb_corpus_clean <- tm_map(nb_corpus_clean, stripWhitespace);
doc_matrix = DocumentTermMatrix(nb_corpus_clean);
return(doc_matrix);
}
#3 -CONVERTING THE FACTOR FROM STRING TO INTEGER - NAIVE BAYES MODEL REQUIREMENT
convert_counts <- function(x){
x <- ifelse(x > 0, "Yes", "No")
}
###################################### PREPROCESSING - ALL MODELS
# fake_news_data = read.csv2('train-3.csv', sep = ',', stringsAsFactors = F)
fake_news_data = read_csv('Reduced Data/train-3.csv')
processed.data <- dataPreprocessing(fake_news_data)
processed.label <- processed.data %>% select('label')
#splitting the data and getting the variables ready for the models
smp_size <- floor(0.75 * nrow(processed.data))
train_index <- sample(seq_len(nrow(processed.data)), size = smp_size)
##train variables
train.input <- processed.data[train_index, ] %>% select('news')
train.label <- processed.data[train_index, ] %>% select('label')
train.data  <- processed.data[train_index, ]
##test variables
test.input <- processed.data[-train_index, ] %>% select('news')
test.label <- processed.data[-train_index, ] %>% select('label')
test.data  <- processed.data[-train_index, ]
x <- train.input
y <- train.label
# Mixed event models
dist <- fastNaiveBayes::fastNaiveBayes.detect_distribution(x, nrows = nrow(x))
print(dist)
mod <- fastNaiveBayes.mixed(x,y,laplace = 1)
pred <- predict(mod, newdata = x)
mean(pred!=y)
# Bernoulli only
vars <- c(dist$bernoulli, dist$multinomial)
newx <- x[,vars]
for(i in 1:ncol(newx)){
newx[[i]] <- as.factor(newx[[i]])
}
###################################### PREPROCESSING - ALL MODELS
# fake_news_data = read.csv2('train-3.csv', sep = ',', stringsAsFactors = F)
fake_news_data = read_csv('Reduced Data/train-3.csv')
###################################### PREPROCESSING - ALL MODELS
# fake_news_data = read.csv2('train-3.csv', sep = ',', stringsAsFactors = F)
fake_news_data = read.csv('Reduced Data/train-3.csv')
processed.data <- dataPreprocessing(fake_news_data)
processed.label <- processed.data %>% select('label')
#splitting the data and getting the variables ready for the models
smp_size <- floor(0.75 * nrow(processed.data))
train_index <- sample(seq_len(nrow(processed.data)), size = smp_size)
##train variables
train.input <- processed.data[train_index, ] %>% select('news')
train.label <- processed.data[train_index, ] %>% select('label')
train.data  <- processed.data[train_index, ]
##test variables
test.input <- processed.data[-train_index, ] %>% select('news')
test.label <- processed.data[-train_index, ] %>% select('label')
test.data  <- processed.data[-train_index, ]
processed.data <- dataPreprocessing(fake_news_data)
library(tidyr)
processed.data <- dataPreprocessing(fake_news_data)
processed.label <- processed.data %>% select('label')
library(dplyr)
processed.label <- processed.data %>% select('label')
#splitting the data and getting the variables ready for the models
smp_size <- floor(0.75 * nrow(processed.data))
train_index <- sample(seq_len(nrow(processed.data)), size = smp_size)
##train variables
train.input <- processed.data[train_index, ] %>% select('news')
train.label <- processed.data[train_index, ] %>% select('label')
train.data  <- processed.data[train_index, ]
##test variables
test.input <- processed.data[-train_index, ] %>% select('news')
test.label <- processed.data[-train_index, ] %>% select('label')
test.data  <- processed.data[-train_index, ]
View(test.label)
x <- train.input
y <- train.label
# Mixed event models
dist <- fastNaiveBayes::fastNaiveBayes.detect_distribution(x, nrows = nrow(x))
print(dist)
mod <- fastNaiveBayes.mixed(x,y,laplace = 1)
pred <- predict(mod, newdata = x)
mean(pred!=y)
# Mixed event models
dist <- fastNaiveBayes::fastNaiveBayes.detect_distribution(x, nrows = nrow(x))
# Mixed event models
dist <- fastNaiveBayes::fnb.detect_distribution(x, nrows = nrow(x))
library(e1071)
# Mixed event models
dist <- fastNaiveBayes::fnb.detect_distribution(x, nrows = nrow(x))
library(fastNaiveBayes)
library(tidyr)
library(dplyr)
library(e1071)
###################################### DEFINING FUNCTIONS
#1 - PREPROCESSING FUNCTION - USED BY ALL MODELS
dataPreprocessing <- function(df) {
df_train <- data.frame('id' = df$tid2 ,
'news' = df$title2_en,
'label' = df$label);
df_train_unique <-  unique(df_train);
df_train_unique$label <- as.factor(df_train_unique$label);
train_label <- pivot_wider(df_train_unique,
id_cols = c('id','news'),
names_from = 'label',
values_from ='label');
train_label <- as.data.frame(train_label);
train_label$label <- NA;
#It provide the final label. If agreed->fake news, else not fake
for (i in 1:length(train_label$id)){
if(!is.na(train_label$agreed[i])) {
train_label$label[i] <-'fake';
}
else {
train_label$label[i] <-'not fake';
}
}
train_label$label[train_label$label=='not fake'] <- as.integer(0);
train_label$label[train_label$label=='fake'] <- as.integer(1);
train_label_final <- train_label[c('id', 'news','label')];
train_label_final$news <- as.character(train_label_final$news);
train_label_final$label  <- factor(train_label_final$label)
return(train_label_final);
}
#2-  Creating Corpus and Generating DTM - NAIVE BAYES AND LOGISTIC REGRESSION REQUIREMENT
createCorpusAndDTM <- function(dataset) {
nb_corpus <- VCorpus(VectorSource(dataset$news));
nb_corpus_clean <- tm_map(nb_corpus, content_transformer(tolower));
nb_corpus_clean <- tm_map(nb_corpus_clean, content_transformer(removeNumbers));
nb_corpus_clean <- tm_map(nb_corpus_clean, removePunctuation);
nb_corpus_clean <- tm_map(nb_corpus_clean, removeWords,stopwords());
nb_corpus_clean <- tm_map(nb_corpus_clean, stemDocument);
nb_corpus_clean <- tm_map(nb_corpus_clean, stripWhitespace);
doc_matrix = DocumentTermMatrix(nb_corpus_clean);
return(doc_matrix);
}
#3 -CONVERTING THE FACTOR FROM STRING TO INTEGER - NAIVE BAYES MODEL REQUIREMENT
convert_counts <- function(x){
x <- ifelse(x > 0, "Yes", "No")
}
###################################### PREPROCESSING - ALL MODELS
# fake_news_data = read.csv2('train-3.csv', sep = ',', stringsAsFactors = F)
fake_news_data = read.csv('Reduced Data/train-3.csv')
processed.data <- dataPreprocessing(fake_news_data)
processed.label <- processed.data %>% select('label')
#splitting the data and getting the variables ready for the models
smp_size <- floor(0.75 * nrow(processed.data))
train_index <- sample(seq_len(nrow(processed.data)), size = smp_size)
##train variables
train.input <- processed.data[train_index, ] %>% select('news')
train.label <- processed.data[train_index, ] %>% select('label')
train.data  <- processed.data[train_index, ]
##test variables
test.input <- processed.data[-train_index, ] %>% select('news')
test.label <- processed.data[-train_index, ] %>% select('label')
test.data  <- processed.data[-train_index, ]
###################################### NAIVE BAYES MODEL
#Theory
{
'''
Why is Naive Bayes “naive”? Naive Bayes is “naive” because of its strong independence assumptions. It assumes that all features are equally important and that all features are independent. If you think of n-grams and compare unigrams and bigrams, you can intuitively understand why the last assumption is a strong assumption. A unigram counts each word as a gram (“I” “like” “walking” “in” “the” “sun”) whereas a bigram counts two words as a gram (“I like” “like walking” “walking in” “in the” “the sun”).
However, even when the assumptions are not fully met, Naive Bayes still performs well.
'''
}
## PROCESSING
# DTM
dtm <- createCorpusAndDTM(processed.data)
# split the document-term matrix
dtm.train <- dtm[1:smp_size, ]
dtm.test <- dtm[smp_size:nrow(processed.data), ]
freq_terms = findFreqTerms(dtm.train, 5)
dtm_freq.train <- dtm.train[, freq_terms]
dtm_freq.test <- dtm.test[, freq_terms]
reduced_dtm.train <- apply(dtm_freq.train, MARGIN=2, convert_counts)
reduced_dtm.test  <- apply(dtm_freq.test, MARGIN=2, convert_counts)
x <- dtm_freq.train
y <- train.label
library(fastNaiveBayes)
library(tidyr)
library(dplyr)
library(e1071)
###################################### DEFINING FUNCTIONS
#1 - PREPROCESSING FUNCTION - USED BY ALL MODELS
dataPreprocessing <- function(df) {
df_train <- data.frame('id' = df$tid2 ,
'news' = df$title2_en,
'label' = df$label);
df_train_unique <-  unique(df_train);
df_train_unique$label <- as.factor(df_train_unique$label);
train_label <- pivot_wider(df_train_unique,
id_cols = c('id','news'),
names_from = 'label',
values_from ='label');
train_label <- as.data.frame(train_label);
train_label$label <- NA;
#It provide the final label. If agreed->fake news, else not fake
for (i in 1:length(train_label$id)){
if(!is.na(train_label$agreed[i])) {
train_label$label[i] <-'fake';
}
else {
train_label$label[i] <-'not fake';
}
}
train_label$label[train_label$label=='not fake'] <- as.integer(0);
train_label$label[train_label$label=='fake'] <- as.integer(1);
train_label_final <- train_label[c('id', 'news','label')];
train_label_final$news <- as.character(train_label_final$news);
train_label_final$label  <- factor(train_label_final$label)
return(train_label_final);
}
#2-  Creating Corpus and Generating DTM - NAIVE BAYES AND LOGISTIC REGRESSION REQUIREMENT
createCorpusAndDTM <- function(dataset) {
nb_corpus <- VCorpus(VectorSource(dataset$news));
nb_corpus_clean <- tm_map(nb_corpus, content_transformer(tolower));
nb_corpus_clean <- tm_map(nb_corpus_clean, content_transformer(removeNumbers));
nb_corpus_clean <- tm_map(nb_corpus_clean, removePunctuation);
nb_corpus_clean <- tm_map(nb_corpus_clean, removeWords,stopwords());
nb_corpus_clean <- tm_map(nb_corpus_clean, stemDocument);
nb_corpus_clean <- tm_map(nb_corpus_clean, stripWhitespace);
doc_matrix = DocumentTermMatrix(nb_corpus_clean);
return(doc_matrix);
}
#3 -CONVERTING THE FACTOR FROM STRING TO INTEGER - NAIVE BAYES MODEL REQUIREMENT
convert_counts <- function(x){
x <- ifelse(x > 0, "Yes", "No")
}
###################################### PREPROCESSING - ALL MODELS
# fake_news_data = read.csv2('train-3.csv', sep = ',', stringsAsFactors = F)
fake_news_data = read.csv('Reduced Data/train-3.csv')
processed.data <- dataPreprocessing(fake_news_data)
processed.label <- processed.data %>% select('label')
#splitting the data and getting the variables ready for the models
smp_size <- floor(0.75 * nrow(processed.data))
train_index <- sample(seq_len(nrow(processed.data)), size = smp_size)
##train variables
train.input <- processed.data[train_index, ] %>% select('news')
train.label <- processed.data[train_index, ] %>% select('label')
train.data  <- processed.data[train_index, ]
##test variables
test.input <- processed.data[-train_index, ] %>% select('news')
test.label <- processed.data[-train_index, ] %>% select('label')
test.data  <- processed.data[-train_index, ]
###################################### NAIVE BAYES MODEL
#Theory
{
# Why is Naive Bayes “naive”? Naive Bayes is “naive” because of its strong independence assumptions. It assumes that all features are equally important and that all features are independent. If you think of n-grams and compare unigrams and bigrams, you can intuitively understand why the last assumption is a strong assumption. A unigram counts each word as a gram (“I” “like” “walking” “in” “the” “sun”) whereas a bigram counts two words as a gram (“I like” “like walking” “walking in” “in the” “the sun”).
#
# However, even when the assumptions are not fully met, Naive Bayes still performs well.
}
## PROCESSING
# DTM
dtm <- createCorpusAndDTM(processed.data)
# split the document-term matrix
dtm.train <- dtm[1:smp_size, ]
dtm.test <- dtm[smp_size:nrow(processed.data), ]
freq_terms = findFreqTerms(dtm.train, 5)
dtm_freq.train <- dtm.train[, freq_terms]
dtm_freq.test <- dtm.test[, freq_terms]
reduced_dtm.train <- apply(dtm_freq.train, MARGIN=2, convert_counts)
reduced_dtm.test  <- apply(dtm_freq.test, MARGIN=2, convert_counts)
x <- dtm_freq.train
y <- train.label
library(tm)
## PROCESSING
# DTM
dtm <- createCorpusAndDTM(processed.data)
# split the document-term matrix
dtm.train <- dtm[1:smp_size, ]
dtm.test <- dtm[smp_size:nrow(processed.data), ]
freq_terms = findFreqTerms(dtm.train, 5)
dtm_freq.train <- dtm.train[, freq_terms]
dtm_freq.test <- dtm.test[, freq_terms]
reduced_dtm.train <- apply(dtm_freq.train, MARGIN=2, convert_counts)
reduced_dtm.test  <- apply(dtm_freq.test, MARGIN=2, convert_counts)
x <- dtm_freq.train
y <- train.label
# Mixed event models
dist <- fastNaiveBayes::fnb.detect_distribution(x, nrows = nrow(x))
View(dist)
#x <- dtm_freq.train
x <- train.input
# Mixed event models
dist <- fastNaiveBayes::fnb.detect_distribution(x, nrows = nrow(x))
x <- dtm_freq.train
y <- train.label
# Mixed event models
dist <- fastNaiveBayes::fnb.detect_distribution(x, nrows = nrow(x))
#dist <- fastNaiveBayes::fastNaiveBayes.detect_distribution(x, nrows = nrow(x))
print(dist)
mod <- fastNaiveBayes.mixed(x,y,laplace = 1)
mod <- fastNaiveBayes.mixed(x,y,laplace = 1)
View(dist)
nb_multinomial <- fastNaiveBayes::fnb.multinomial(x,y)
nb_multinomial <- fastNaiveBayes::fnb.multinomial(train.input,train.label)
View(train.label)
mod <- fastNaiveBayes::fastNaiveBayes(x = train.input,y = train.label, distribution = fnb.detect_distribution(x))
length(train.input)
length(train.label)
#splitting the data and getting the variables ready for the models
smp_size <- floor(0.75 * nrow(processed.data))
train_index <- sample(seq_len(nrow(processed.data)), size = smp_size)
##train variables
train.input <- processed.data[train_index, ] %>% select('news')
train.label <- processed.data[train_index, ] %>% select('label')
train.data  <- processed.data[train_index, ]
library(fastNaiveBayes)
library(tidyr)
library(dplyr)
library(e1071)
library(tm)
###################################### DEFINING FUNCTIONS
#1 - PREPROCESSING FUNCTION - USED BY ALL MODELS
dataPreprocessing <- function(df) {
df_train <- data.frame('id' = df$tid2 ,
'news' = df$title2_en,
'label' = df$label);
df_train_unique <-  unique(df_train);
df_train_unique$label <- as.factor(df_train_unique$label);
train_label <- pivot_wider(df_train_unique,
id_cols = c('id','news'),
names_from = 'label',
values_from ='label');
train_label <- as.data.frame(train_label);
train_label$label <- NA;
#It provide the final label. If agreed->fake news, else not fake
for (i in 1:length(train_label$id)){
if(!is.na(train_label$agreed[i])) {
train_label$label[i] <-'fake';
}
else {
train_label$label[i] <-'not fake';
}
}
train_label$label[train_label$label=='not fake'] <- as.integer(0);
train_label$label[train_label$label=='fake'] <- as.integer(1);
train_label_final <- train_label[c('id', 'news','label')];
train_label_final$news <- as.character(train_label_final$news);
train_label_final$label  <- factor(train_label_final$label)
return(train_label_final);
}
#2-  Creating Corpus and Generating DTM - NAIVE BAYES AND LOGISTIC REGRESSION REQUIREMENT
createCorpusAndDTM <- function(dataset) {
nb_corpus <- VCorpus(VectorSource(dataset$news));
nb_corpus_clean <- tm_map(nb_corpus, content_transformer(tolower));
nb_corpus_clean <- tm_map(nb_corpus_clean, content_transformer(removeNumbers));
nb_corpus_clean <- tm_map(nb_corpus_clean, removePunctuation);
nb_corpus_clean <- tm_map(nb_corpus_clean, removeWords,stopwords());
nb_corpus_clean <- tm_map(nb_corpus_clean, stemDocument);
nb_corpus_clean <- tm_map(nb_corpus_clean, stripWhitespace);
doc_matrix = DocumentTermMatrix(nb_corpus_clean);
return(doc_matrix);
}
#3 -CONVERTING THE FACTOR FROM STRING TO INTEGER - NAIVE BAYES MODEL REQUIREMENT
convert_counts <- function(x){
x <- ifelse(x > 0, "Yes", "No")
}
###################################### PREPROCESSING - ALL MODELS
# fake_news_data = read.csv2('train-3.csv', sep = ',', stringsAsFactors = F)
fake_news_data = read.csv('Reduced Data/train-3.csv')
processed.data <- dataPreprocessing(fake_news_data)
processed.label <- processed.data %>% select('label')
#splitting the data and getting the variables ready for the models
smp_size <- floor(0.75 * nrow(processed.data))
train_index <- sample(seq_len(nrow(processed.data)), size = smp_size)
##train variables
train.input <- processed.data[train_index, ] %>% select('news')
train.label <- processed.data[train_index, ] %>% select('label')
train.data  <- processed.data[train_index, ]
##test variables
test.input <- processed.data[-train_index, ] %>% select('news')
test.label <- processed.data[-train_index, ] %>% select('label')
test.data  <- processed.data[-train_index, ]
length(train.input)
View(train.input)
train.input <- as.data.frame(train.input)
length(train.input)
##train variables
train.input <- as.data.frame(processed.data[train_index, ]) %>% select('news')
##train variables
train.input <- processed.data[train_index, ] %>% select('news')
lenght(train.input$news)
length(train.input$news)
length(train.label$label)
train.input <- processed.data[train_index, ] %>% select('news')
train.input <- train.input$news
train.label <- processed.data[train_index, ] %>% select('label')
train.label <- train.label$label
mod <- fastNaiveBayes::fastNaiveBayes(x = train.input,y = train.label, distribution = fnb.detect_distribution(x))
